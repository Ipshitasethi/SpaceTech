import streamlit as st
from ultralytics import YOLO
from PIL import Image
import numpy as np
import cv2
import os
from datetime import datetime
import pyttsx3
import threading
from collections import defaultdict

import yaml


# Get absolute path to config.yaml no matter where script is run from
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
CONFIG_PATH = os.path.join(BASE_DIR, "config", "config.yaml")
RESULTS_PATH = os.path.join(BASE_DIR, "runs")

print("BASE_DIR:", BASE_DIR)

# Ensure results directory exists
os.makedirs(RESULTS_PATH, exist_ok=True)



# Load configuration file
with open(CONFIG_PATH, "r") as f:
    CONFIG = yaml.safe_load(f)

EXAMPLES_PATH = os.path.abspath(os.path.join(BASE_DIR, CONFIG["paths"]["examples"]))
MODELS_PATH = os.path.abspath(os.path.join(BASE_DIR, CONFIG["paths"]["models"]))






# --- PAGE CONFIG ---
st.set_page_config(page_title="AstraSight", page_icon="üöÄ", layout="wide")

# --- FACT DATA ---
object_facts = {
    "oxygentank": {
        "what": "Stores breathable oxygen for astronauts.",
        "use": "Used during EVAs and emergencies to provide life support."
    },
    "toolbox": {
        "what": "Contains essential tools for space repairs.",
        "use": "Used by astronauts to fix panels and cables during spacewalks."
    },
    "fireextinguisher": {
        "what": "Used to suppress fires in microgravity.",
        "use": "Activated in the event of fire to protect crew."
    },
    "default": {
        "what": "No data available.",
        "use": "No usage instructions found."
    }
}

# --- INIT STATE ---
spoken_classes = set()
object_counts = defaultdict(int)

# --- SPEECH THREADING ---
def speak_threaded(text):
    def run_speech():
        engine = pyttsx3.init()
        engine.say(text)
        engine.runAndWait()
    threading.Thread(target=run_speech).start()

def speak_once(label):
    if enable_speech and label not in spoken_classes:
        spoken_classes.add(label)
        speak_threaded(label)

# --- RESIZE IMAGE ---
def resize_frame(frame, width=640):
    h, w = frame.shape[:2]
    scale = width / w
    return cv2.resize(frame, (int(w * scale), int(h * scale)))

# --- STATUS DISPLAY ---
def danger_status(detected):
    if "fireextinguisher" in detected:
        return "üü¢ Safe - Fire extinguisher present."
    elif "toolbox" in detected:
        return "üü° Caution - Tools in area."
    elif "oxygentank" not in detected:
        return "üî¥ Danger - Oxygen supply not found!"
    else:
        return "üü° Monitor - Ensure equipment visibility."

# --- SIDEBAR ---
with st.sidebar:
    st.image("https://images.unsplash.com/photo-1446776811953-b23d57bd21aa", width=250)
    model_files = [f for f in os.listdir(MODELS_PATH) if f.endswith((".pt", ".onnx"))]
    model_name = st.selectbox("üì¶ Select YOLO Model", model_files)
    confidence = st.slider("üéØ Confidence Threshold", 0.1, 1.0, 0.25, step=0.01)
    enable_speech = st.checkbox("üó£ Enable Voice Feedback", value=True)
    st.markdown("---")

    st.markdown("""### ‚ú® AstraSight ‚Äì Key Features
- üîç Real-Time Object Detection using YOLOv8
- üß† Multi-Class Tracking for mission-critical items
- üîä Audio Alerts with confidence scores
- üñºÔ∏è Live Feed & Image Upload Support
- üìä Object Count & Frame Logging

""")
    st.markdown("### ü§ñ Team Name: Visionaries11:11")
    st.markdown("""
    Members:  
    - Ipshita Sethi  
    - Saumya Sudha  
    - Gazal Sindhwani
    - Muskan Goel  
    - Lavanya Sharma 
    - Khushi Ajwani 
    """)

model_path = os.path.join(MODELS_PATH, model_name)
model = YOLO(model_path)
# st.write(f"Model path: {model_path}")
# st.write(f"File exists: {os.path.exists(model_path)}")
# st.write(f"File size: {os.path.getsize(model_path) if os.path.exists(model_path) else 'N/A'} bytes")


# --- LOAD MODEL ---
@st.cache_resource(show_spinner=False)
def load_model(path, model_name):
    import torch
    from ultralytics.nn.tasks import DetectionModel

    # Tell PyTorch it's safe to deserialize DetectionModel from your checkpoint
    torch.serialization.add_safe_globals([DetectionModel])
    
    print(f"Loading model '{model_name}' from: {path}")
    return YOLO(path)





# --- HEADER & BANNER ---
st.image("https://images.unsplash.com/photo-1581090700227-1e7e54bc59f4", use_container_width=True)
st.markdown("""
    <h1 style='text-align: center; color: #FF4B4B;'>üöÄ AstraSight</h1>
    <h4 style='text-align: center; color: gray;'>Real-Time Safety in Zero Gravity</h4>
""", unsafe_allow_html=True)

st.markdown("Detects toolbox, oxygen tank, and fire extinguisher using YOLOv8")

# --- INPUT MODE ---
option = st.radio("Select input method:", ("Upload Image", "Use Example Image", "Use Webcam"))

# --- WEBCAM DETECTION ---
if option == "Use Webcam":
    run = st.checkbox("üì∏ Start Camera")
    FRAME_WINDOW = st.image([])
    camera = cv2.VideoCapture(0)
    save_button = st.button("üíæ Save Current Frame")

    while run:
        ret, frame = camera.read()
        if not ret:
            st.error("Webcam not accessible.")
            break

        frame = resize_frame(frame)
        results = model.predict(frame, conf=confidence, iou=0.5)
        boxes = results[0].boxes

        detected_labels = []

        if len(boxes) == 0:
            spoken_classes.clear()

        for box in boxes:
            coords = box.xyxy[0].tolist()
            class_id = int(box.cls[0])
            conf_score = box.conf[0].item()
            class_name = model.names[class_id].lower()
            label_text = f"{class_name} ({conf_score:.2f})"
            speak_label = f"{class_name} detected with {conf_score:.0%} confidence"

            detected_labels.append(class_name)
            object_counts[class_name] += 1
            speak_once(speak_label)

            cv2.rectangle(frame, (int(coords[0]), int(coords[1])), (int(coords[2]), int(coords[3])), (0, 255, 0), 2)
            cv2.putText(frame, label_text, (int(coords[0]), int(coords[1]) - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        FRAME_WINDOW.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

        if save_button:
            filename = os.path.join(RESULTS_PATH,f"saved_frame_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg")
            cv2.imwrite(filename, frame)
            st.success(f"Frame saved as {filename}")

        if detected_labels:
            st.markdown(f"### üõ° Status: {danger_status(detected_labels)}")

# --- IMAGE UPLOAD DETECTION ---
elif option == "Upload Image":
    uploaded_file = st.file_uploader("üì§ Upload an image", type=["jpg", "jpeg", "png"])

    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("RGB")
        img_array = resize_frame(np.array(image))

        with st.spinner("üîç Running detection..."):
            results = model.predict(img_array, conf=confidence, iou=0.5)
            boxes = results[0].boxes

        detected_labels = []

        if len(boxes) == 0:
            st.warning("‚ùó No objects detected.")
            spoken_classes.clear()

        for box in boxes:
            coords = box.xyxy[0].tolist()
            class_id = int(box.cls[0])
            conf_score = box.conf[0].item()
            class_name = model.names[class_id].lower()
            label_text = f"{class_name} ({conf_score:.2f})"
            speak_label = f"{class_name} detected with {conf_score:.0%} confidence"

            detected_labels.append(class_name)
            object_counts[class_name] += 1
            speak_once(speak_label)

            cv2.rectangle(img_array, (int(coords[0]), int(coords[1])), (int(coords[2]), int(coords[3])), (0, 255, 0), 2)
            cv2.putText(img_array, label_text, (int(coords[0]), int(coords[1]) - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        st.image(img_array, caption="üõ∞ Detection Results", use_container_width=True)

        if st.button("üíæ Save Output Image"):
            output_filename = os.path.join(RESULTS_PATH, f"detected_image_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg")
            cv2.imwrite(output_filename, cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR))
            st.success(f"Image saved as {output_filename}")

        st.markdown("### üìã Detected Objects")

        for label in detected_labels:
            conf_list = [box.conf[0].item() for box in boxes if model.names[int(box.cls[0])].lower() == label]
            if conf_list:
                st.write(f"- {label.title()} ‚Äî Confidence: {conf_list[0]:.2f}")
            obj = object_facts.get(label.lower(), object_facts["default"])
            st.markdown(f"<small>üß† What it is: {obj['what']}</small>", unsafe_allow_html=True)
            st.markdown(f"<small>üõ† How it‚Äôs used: {obj['use']}</small>", unsafe_allow_html=True)

        st.markdown(f"### üõ° Status: {danger_status(detected_labels)}")


elif option == "Use Example Image":
    import glob
    
    # Load example image file paths
    example_files = glob.glob(os.path.join(EXAMPLES_PATH, "*.*"))  # All files in example folder
    
    # Let user select from example images by filename
    example_names = [os.path.basename(f) for f in example_files]
    
    selected_example = st.selectbox("Select an example image", example_names)
    
    if selected_example:
        example_path = os.path.join(EXAMPLES_PATH, selected_example)
        image = Image.open(example_path).convert("RGB")
        img_array = resize_frame(np.array(image))
        
        with st.spinner("üîç Running detection on example image..."):
            results = model.predict(img_array, conf=confidence, iou=0.5)
            boxes = results[0].boxes
        
        detected_labels = []
        if len(boxes) == 0:
            st.warning("‚ùó No objects detected.")
            spoken_classes.clear()

        for box in boxes:
            coords = box.xyxy[0].tolist()
            class_id = int(box.cls[0])
            conf_score = box.conf[0].item()
            class_name = model.names[class_id].lower()
            label_text = f"{class_name} ({conf_score:.2f})"
            speak_label = f"{class_name} detected with {conf_score:.0%} confidence"

            detected_labels.append(class_name)
            object_counts[class_name] += 1
            speak_once(speak_label)

            cv2.rectangle(img_array, (int(coords[0]), int(coords[1])), (int(coords[2]), int(coords[3])), (0, 255, 0), 2)
            cv2.putText(img_array, label_text, (int(coords[0]), int(coords[1]) - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        st.image(img_array, caption=f"üõ∞ Detection Results on {selected_example}", use_container_width=True)

        if st.button("üíæ Save Output Image"):
            output_filename = os.path.join(RESULTS_PATH, f"detected_image_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg")
            cv2.imwrite(output_filename, cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR))
            st.success(f"Image saved as {output_filename}")

        st.markdown("### üìã Detected Objects")
        for label in detected_labels:
            conf_list = [box.conf[0].item() for box in boxes if model.names[int(box.cls[0])].lower() == label]
            if conf_list:
                st.write(f"- {label.title()} ‚Äî Confidence: {conf_list[0]:.2f}")
            obj = object_facts.get(label.lower(), object_facts["default"])
            st.markdown(f"<small>üß† What it is: {obj['what']}</small>", unsafe_allow_html=True)
            st.markdown(f"<small>üõ† How it‚Äôs used: {obj['use']}</small>", unsafe_allow_html=True)

        st.markdown(f"### üõ° Status: {danger_status(detected_labels)}")



# --- FOOTER ---
st.markdown("---")
st.markdown("üõ† Developed for SIH 2025")
